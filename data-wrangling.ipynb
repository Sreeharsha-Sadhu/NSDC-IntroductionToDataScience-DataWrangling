{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Wrangling and Missing Value Handling**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "### Why is Data Wrangling Important?\n",
    "- **Real-world data** is often messy, containing missing values, inconsistencies, and irrelevant features.\n",
    "- **Unprocessed data** can lead to inaccurate insights and unreliable models.\n",
    "- **Effective data wrangling** transforms raw data into a structured, clean format suitable for analysis and machine learning.\n",
    "\n",
    "### **Key Steps in Data Wrangling:**\n",
    "1. **Data Cleaning** â€“ Handling missing values, removing outliers, and standardizing formats.\n",
    "2. **Data Transformation** â€“ Encoding categorical variables, normalizing numerical data, and feature engineering.\n",
    "3. **Data Integration** â€“ Merging, reshaping, and aggregating data for better analysis.\n",
    "\n",
    "## **Dataset Used for Demonstration**\n",
    "We will use a **synthetic dataset** designed to teach **Data Wrangling** techniques by addressing common data issues. The dataset contains:\n",
    "\n",
    "### **1. Numerical Features**\n",
    "- **Age** â€“ Contains missing values and outliers (e.g., unrealistic values).\n",
    "- **Salary** â€“ Has missing values that require imputation.\n",
    "- **Work Experience** â€“ Some missing entries, demonstrating handling techniques.\n",
    "- **Job Satisfaction Score** â€“ Skewed distribution, useful for transformations.\n",
    "- **Customer Satisfaction Rating** â€“ Ranges from 1 to 10, useful for normalization.\n",
    "\n",
    "### **2. Categorical Features**\n",
    "- **Name** â€“ Contains duplicates and inconsistencies (e.g., different cases, extra spaces).\n",
    "- **Department** â€“ Includes typos and inconsistent categories.\n",
    "- **Education Level** â€“ Ordinal categorical variable requiring encoding.\n",
    "- **Remote Work** â€“ Binary categorical feature useful for one-hot encoding.\n",
    "- **Performance Score** â€“ Imbalanced target variable, demonstrating resampling techniques.\n",
    "\n",
    "### **3. Date and Currency Fields**\n",
    "- **Join Date** â€“ Stored in mixed formats, demonstrating date parsing.\n",
    "- **Bonus** â€“ Contains currency symbols (`$`, `â‚¬`), requiring conversion to numeric values.\n",
    "\n",
    "### **4. Key Data Wrangling Challenges Covered**\n",
    "- **Handling missing values** in Salary, Work Experience, and Age using imputation techniques.\n",
    "- **Dealing with outliers** in Age by capping, removing, or transforming values.\n",
    "- **Fixing inconsistencies** in categorical data through text standardization and deduplication.\n",
    "- **Encoding categorical variables** for machine learning compatibility.\n",
    "- **Normalizing and transforming numerical features** to improve data distributions.\n",
    "- **Addressing imbalanced target variables** through resampling techniques.\n",
    "\n",
    "This dataset is structured to provide hands-on experience in **Data Wrangling**, helping students learn essential techniques for real-world data preprocessing.\n",
    "\n",
    "Let's start by loading the dataset. ðŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T06:37:24.529790Z",
     "start_time": "2025-02-19T06:37:22.947164Z"
    }
   },
   "source": [
    "\n",
    "import numpy as np\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"./data/data_wrangling_dataset.csv\")\n",
    "\n",
    "# Display first few rows\n",
    "dataset.head()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   ID     Name  Age   Salary   Join_Date Department Education_Level  \\\n",
       "0   1     Hank   39      NaN  2015-01-01         IT        Master's   \n",
       "1   2      Eve   33  63198.0  2015-01-02         HR     High School   \n",
       "2   3    David   41  43065.0  2015-01-03    Finance      Bachelor's   \n",
       "3   4    David   50  65048.0  2015-01-04        IT       Bachelor's   \n",
       "4   5  Charlie   32  80992.0  2015-01-05         HR     High School   \n",
       "\n",
       "   Work_Experience  Performance_Score   Bonus  Remote_Work  Job_Satisfaction  \\\n",
       "0              NaN                  5  â‚¬$6566            1          3.668445   \n",
       "1             23.0                  4    4969            1          0.423328   \n",
       "2             16.0                  4    6420            0          1.759563   \n",
       "3             20.0                  3    1607            0          3.000364   \n",
       "4              1.0                  4    6674            0          2.425639   \n",
       "\n",
       "   Customer_Satisfaction  \n",
       "0                      7  \n",
       "1                      3  \n",
       "2                      6  \n",
       "3                      6  \n",
       "4                      2  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Join_Date</th>\n",
       "      <th>Department</th>\n",
       "      <th>Education_Level</th>\n",
       "      <th>Work_Experience</th>\n",
       "      <th>Performance_Score</th>\n",
       "      <th>Bonus</th>\n",
       "      <th>Remote_Work</th>\n",
       "      <th>Job_Satisfaction</th>\n",
       "      <th>Customer_Satisfaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Hank</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>IT</td>\n",
       "      <td>Master's</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>â‚¬$6566</td>\n",
       "      <td>1</td>\n",
       "      <td>3.668445</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Eve</td>\n",
       "      <td>33</td>\n",
       "      <td>63198.0</td>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>HR</td>\n",
       "      <td>High School</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4969</td>\n",
       "      <td>1</td>\n",
       "      <td>0.423328</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>David</td>\n",
       "      <td>41</td>\n",
       "      <td>43065.0</td>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>Finance</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6420</td>\n",
       "      <td>0</td>\n",
       "      <td>1.759563</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>David</td>\n",
       "      <td>50</td>\n",
       "      <td>65048.0</td>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>IT</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1607</td>\n",
       "      <td>0</td>\n",
       "      <td>3.000364</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>32</td>\n",
       "      <td>80992.0</td>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>HR</td>\n",
       "      <td>High School</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6674</td>\n",
       "      <td>0</td>\n",
       "      <td>2.425639</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **1. Data Cleaning**\n",
    "\n",
    "## **1.1 Handling Missing Values**\n",
    "\n",
    "### **Why do Missing Values Occur?**\n",
    "- **Data collection errors** (e.g., sensor malfunctions, survey non-responses).\n",
    "- **Human errors** (e.g., incorrect data entry).\n",
    "- **Different data sources** (some sources may not have certain attributes).\n",
    "\n",
    "### **Methods to Handle Missing Values:**\n",
    "\n",
    "1. **Deletion (Dropping Missing Values):**\n",
    "   - **When to use?** If only a small percentage of data is missing.\n",
    "   - **Drawback:** Can result in loss of valuable data.\n",
    "\n",
    "2. **Imputation (Filling Missing Values):**\n",
    "   - **Mean/Median Imputation** (for numerical data) â€“ works well if data is normally distributed.\n",
    "   - **Mode Imputation** (for categorical data) â€“ replaces missing values with the most frequent value.\n",
    "   - **Forward/Backward Fill** (for time-series data) â€“ fills missing values based on previous or next observations.\n",
    "   - **KNN Imputation** â€“ predicts missing values based on similar data points.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T06:37:27.625033Z",
     "start_time": "2025-02-19T06:37:27.621786Z"
    }
   },
   "cell_type": "code",
   "source": "df = dataset.copy(deep=True)",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T06:37:35.581188Z",
     "start_time": "2025-02-19T06:37:35.575543Z"
    }
   },
   "source": [
    "# Check missing values\n",
    "print(\"Missing values before handling:\")\n",
    "print(df.isnull().sum())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before handling:\n",
      "ID                         0\n",
      "Name                       0\n",
      "Age                        0\n",
      "Salary                   167\n",
      "Join_Date                  0\n",
      "Department                 0\n",
      "Education_Level            0\n",
      "Work_Experience          125\n",
      "Performance_Score          0\n",
      "Bonus                      0\n",
      "Remote_Work                0\n",
      "Job_Satisfaction           0\n",
      "Customer_Satisfaction      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Drop rows with missing values (not recommended if data loss is high)\n",
    "df_dropped = df.dropna()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Impute numerical values (Age, Income) using Mean & Median\n",
    "imputer_mean = SimpleImputer(strategy='mean')\n",
    "df['Age'] = imputer_mean.fit_transform(df[['Age']])\n",
    "\n",
    "imputer_median = SimpleImputer(strategy='median')\n",
    "df['Income'] = imputer_median.fit_transform(df[['Income']])\n",
    "\n",
    "# Impute categorical values (City) using Mode\n",
    "imputer_mode = SimpleImputer(strategy='most_frequent')\n",
    "df['City'] = imputer_mode.fit_transform(df[['City']])\n",
    "\n",
    "# Predictive Imputation using KNN (for Age & Income)\n",
    "knn_imputer = KNNImputer(n_neighbors=3)\n",
    "df[['Age', 'Income']] = knn_imputer.fit_transform(df[['Age', 'Income']])\n",
    "\n",
    "\n",
    "# Removing duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Check missing values after handling\n",
    "print(\"Missing values after handling:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Numerical Columns\n",
    "for col in ['Age', 'Salary', 'Work Experience']:\n",
    "    df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "# Categorical Columns\n",
    "for col in ['Department', 'Education Level', 'Remote Work']:\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **1.2 Handling Outliers**\n",
    "\n",
    "### **What are Outliers?**\n",
    "- **Outliers** are extreme values that differ significantly from other observations.\n",
    "- They can be caused by **errors** or **natural variations** in the data.\n",
    "\n",
    "### **Methods to Detect and Remove Outliers:**\n",
    "\n",
    "1. **Z-Score Method**:\n",
    "   - Measures how many standard deviations a data point is from the mean.\n",
    "   - If the absolute Z-score is greater than 3, the value is considered an outlier.\n",
    "\n",
    "2. **Interquartile Range (IQR) Method**:\n",
    "   - Detects outliers by identifying values **outside 1.5 times the IQR**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Z-Score\n",
    "z_scores = np.abs(zscore(df['Age']))\n",
    "df = df[z_scores < 3]  # Removing outliers based on Z-score\n",
    "\n",
    "# Detecting Outliers using IQR\n",
    "Q1 = df['Salary'].quantile(0.25)\n",
    "Q3 = df['Salary'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df['Salary'] = df[(df['Salary'] >= Q1 - 1.5 * IQR) & (df['Salary'] <= Q3 + 1.5 * IQR)]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **1.4 Standardizing Data Formats**\n",
    "\n",
    "### **Why Standardization of Data Formats is Important?**\n",
    "\n",
    "- Ensures Consistency: Data often comes in different formats, making uniform processing essential.\n",
    "\n",
    "- Facilitates Comparisons: Uniform formats enable accurate analysis and computations.\n",
    "\n",
    "- Reduces Errors: Inconsistent formats can lead to misinterpretations and processing issues.\n",
    "\n",
    "- Improves Data Quality: Standardization simplifies data cleaning and validation.\n",
    "\n",
    "### **Common Data Formats to Standardize**\n",
    "\n",
    "1. **Date and Time Standardization:** Standardizing date formats ensures accurate sorting, filtering, and time-based analysis. Using a universally accepted format, such as ISO 8601, enhances consistency.\n",
    "\n",
    "2. **Numeric Data Standardization:** Removing extra characters like currency symbols and ensuring a consistent decimal notation is essential for correct calculations.\n",
    "\n",
    "3. **Categorical Data Standardization:** Variations in categorical values (e.g., different capitalizations or abbreviations) can cause inconsistencies. Standardizing these values improves reliability.\n",
    "\n",
    "4. **Text and String Formatting:** Removing unnecessary spaces, special characters, and ensuring uniform capitalization enhances text processing.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Standardizing date format\n",
    "df['Join Date'] = pd.to_datetime(df['Join Date'], errors='coerce')\n",
    "\n",
    "# Converting currency column to numeric using different approaches\n",
    "df['Bonus'] = df['Bonus'].replace({'\\$': '', 'â‚¬': ''}, regex=True).astype(float)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **2. Data Pre-processing**\n",
    "\n",
    "## **2.1 Encoding Categorical Variables**\n",
    "\n",
    "### **Why do we need Encoding?**\n",
    "- Machine learning models require **numerical data**.\n",
    "- Encoding converts categorical values into numbers.\n",
    "\n",
    "### **Types of Encoding:**\n",
    "\n",
    "1. **Label Encoding**:\n",
    "   - Assigns **a unique integer** to each category.\n",
    "   - **Best for** ordinal categorical data (e.g., Small, Medium, Large).\n",
    "\n",
    "2. **One-Hot Encoding**:\n",
    "   - Creates separate **binary columns** for each category.\n",
    "   - **Best for** nominal categorical data (e.g., Cities, Colors).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical variables using multiple techniques\n",
    "label_encoder = LabelEncoder()\n",
    "df['Education Level'] = label_encoder.fit_transform(df['Education Level'])  # Label Encoding\n",
    "\n",
    "# Alternative method: (One-Hot Encoding)\n",
    "df = pd.get_dummies(df, columns=['Department', 'Remote Work'], drop_first=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **2.1 Handling Imbalanced Datasets**\n",
    "\n",
    "- **Why It Matters**: In imbalanced datasets, one class significantly outweighs others, leading to biased models that favor the dominant class.\n",
    "- **Strategies for Handling Imbalance**:\n",
    "  - **Resampling Methods**: Oversampling the minority class or undersampling the majority class can help balance the dataset.\n",
    "  - **Synthetic Data Generation**: Techniques like SMOTE (Synthetic Minority Over-sampling Technique) generate synthetic examples to balance class distribution.\n",
    "  - **Algorithmic Approaches**: Some machine learning models handle imbalance better by adjusting class weights.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Handling imbalanced datasets\n",
    "X = df.drop(columns=['Performance Score'])\n",
    "y = df['Performance Score']\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "df = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **2.2 Feature Selection**\n",
    "- **Why It Matters**: Irrelevant or low-variance features can add noise and reduce model performance.\n",
    "- **Methods for Feature Selection**:\n",
    "  - **Variance Thresholding**: Removing features with very low variance helps eliminate those that contribute little to predictive power.\n",
    "  - **Correlation Analysis**: Highly correlated features can be redundant and may be removed to simplify the model.\n",
    "  - **Model-Based Selection**: Feature importance"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Feature selection: Dropping low-variance columns manually\n",
    "threshold = 0.01 * (1 - 0.01)\n",
    "low_variance_cols = [col for col in df.columns if df[col].var() < threshold]\n",
    "df = df.drop(columns=low_variance_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **3. Data Transformation**\n",
    "\n",
    "## **3.1 Scaling & Normalization**\n",
    "\n",
    "### **Why Scale Data?**\n",
    "- Ensures that **all features contribute equally** to the model.\n",
    "- Improves performance in algorithms like KNN, SVM, and PCA.\n",
    "\n",
    "### **Methods:**\n",
    "1. **Min-Max Scaling**: Scales data to a range **[0,1]**.\n",
    "2. **Standardization (Z-score)**: Centers data around **mean = 0, std = 1**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "df[['Age', 'Salary', 'Work Experience']] = scaler.fit_transform(df[['Age', 'Salary', 'Work Experience']])\n",
    "\n",
    "\n",
    "# Standardization\n",
    "df[['Age', 'Salary', 'Work Experience']] = StandardScaler().fit_transform(df[['Age', 'Salary', 'Work Experience']])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Log transformation for skewed data using numpy\n",
    "for col in ['Salary', 'Bonus']:\n",
    "    df[col] = np.log1p(df[col])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Feature engineering: Creating new features\n",
    "df['Experience per Year'] = df['Work Experience'] / (df['Age'] + 1)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.to_csv(\"cleaned_synthetic_data.csv\", index=False)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  },
  "prev_pub_hash": "0f2053140168387dde22a52dafe4fef4f297eb98b70601c31c5dbae85ffb8130"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
