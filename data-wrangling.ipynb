{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Wrangling and Missing Value Handling**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "### Why is Data Wrangling Important?\n",
    "- **Real-world data** is often messy, containing missing values, inconsistencies, and irrelevant features.\n",
    "- **Unprocessed data** can lead to inaccurate insights and unreliable models.\n",
    "- **Effective data wrangling** transforms raw data into a structured, clean format suitable for analysis and machine learning.\n",
    "\n",
    "### **Key Steps in Data Wrangling:**\n",
    "1. **Data Cleaning** – Handling missing values, removing outliers, and standardizing formats.\n",
    "2. **Data Transformation** – Encoding categorical variables, normalizing numerical data, and feature engineering.\n",
    "3. **Data Integration** – Merging, reshaping, and aggregating data for better analysis.\n",
    "\n",
    "## **Dataset Used for Demonstration**\n",
    "We will use a **synthetic dataset** designed to teach **Data Wrangling** techniques by addressing common data issues. The dataset contains:\n",
    "\n",
    "### **1. Numerical Features**\n",
    "- **Age** – Contains missing values and outliers (e.g., unrealistic values).\n",
    "- **Salary** – Has missing values that require imputation.\n",
    "- **Work Experience** – Some missing entries, demonstrating handling techniques.\n",
    "- **Job Satisfaction Score** – Skewed distribution, useful for transformations.\n",
    "- **Customer Satisfaction Rating** – Ranges from 1 to 10, useful for normalization.\n",
    "\n",
    "### **2. Categorical Features**\n",
    "- **Name** – Contains duplicates and inconsistencies (e.g., different cases, extra spaces).\n",
    "- **Department** – Includes typos and inconsistent categories.\n",
    "- **Education Level** – Ordinal categorical variable requiring encoding.\n",
    "- **Remote Work** – Binary categorical feature useful for one-hot encoding.\n",
    "- **Performance Score** – Imbalanced target variable, demonstrating resampling techniques.\n",
    "\n",
    "### **3. Date and Currency Fields**\n",
    "- **Join Date** – Stored in mixed formats, demonstrating date parsing.\n",
    "- **Bonus** – Contains currency symbols (`$`, `€`), requiring conversion to numeric values.\n",
    "\n",
    "### **4. Key Data Wrangling Challenges Covered**\n",
    "- **Handling missing values** in Salary, Work Experience, and Age using imputation techniques.\n",
    "- **Dealing with outliers** in Age by capping, removing, or transforming values.\n",
    "- **Fixing inconsistencies** in categorical data through text standardization and deduplication.\n",
    "- **Encoding categorical variables** for machine learning compatibility.\n",
    "- **Normalizing and transforming numerical features** to improve data distributions.\n",
    "- **Addressing imbalanced target variables** through resampling techniques.\n",
    "\n",
    "This dataset is structured to provide hands-on experience in **Data Wrangling**, helping students learn essential techniques for real-world data preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"./data/data_wrangling_dataset.csv\")\n",
    "\n",
    "# Display first few rows\n",
    "dataset.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **1. Data Cleaning**\n",
    "\n",
    "## **1.1 Handling Missing Values**\n",
    "\n",
    "### **Why do Missing Values Occur?**\n",
    "- **Data collection errors** (e.g., sensor malfunctions, survey non-responses).\n",
    "- **Human errors** (e.g., incorrect data entry).\n",
    "- **Different data sources** (some sources may not have certain attributes).\n",
    "\n",
    "### **Methods to Handle Missing Values:**\n",
    "\n",
    "1. **Deletion (Dropping Missing Values):**\n",
    "   - **When to use?** If only a small percentage of data is missing.\n",
    "   - **Drawback:** Can result in loss of valuable data.\n",
    "\n",
    "2. **Imputation (Filling Missing Values):**\n",
    "   - **Mean/Median Imputation** (for numerical data) – works well if data is normally distributed.\n",
    "   - **Mode Imputation** (for categorical data) – replaces missing values with the most frequent value.\n",
    "   - **Forward/Backward Fill** (for time-series data) – fills missing values based on previous or next observations.\n",
    "   - **KNN Imputation** – predicts missing values based on similar data points.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = dataset.copy(deep=True)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check missing values\n",
    "print(\"Missing values before handling:\")\n",
    "print(df.isnull().sum())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Drop rows with missing values (not recommended if data loss is high)\n",
    "df_dropped = df.dropna()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Impute numerical values (Age, Income) using Mean & Median\n",
    "imputer_mean = SimpleImputer(strategy='mean')\n",
    "df['Salary'] = imputer_mean.fit_transform(df[['Salary']])\n",
    "\n",
    "# Predictive Imputation using KNN (for Age & Income)\n",
    "knn_imputer = KNNImputer(n_neighbors=3)\n",
    "df[['Salary', 'Work_Experience']] = knn_imputer.fit_transform(df[['Salary', 'Work_Experience']])\n",
    "\n",
    "\n",
    "# Removing duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Check missing values after handling\n",
    "print(\"Missing values after handling:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Numerical Columns\n",
    "for col in ['Age', 'Salary', 'Work_Experience']:\n",
    "    df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "# Categorical Columns\n",
    "for col in ['Department', 'Education_Level', 'Remote_Work']:\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **1.2 Handling Outliers**\n",
    "\n",
    "### **What are Outliers?**\n",
    "- **Outliers** are extreme values that differ significantly from other observations.\n",
    "- They can be caused by **errors** or **natural variations** in the data.\n",
    "\n",
    "### **Methods to Detect and Remove Outliers:**\n",
    "\n",
    "1. **Z-Score Method**:\n",
    "   - Measures how many standard deviations a data point is from the mean.\n",
    "   - If the absolute Z-score is greater than 3, the value is considered an outlier.\n",
    "\n",
    "2. **Interquartile Range (IQR) Method**:\n",
    "   - Detects outliers by identifying values **outside 1.5 times the IQR**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Using Z-Score\n",
    "z_scores = np.abs(zscore(df['Age']))\n",
    "df = df[z_scores < 3]  # Removing outliers based on Z-score\n",
    "\n",
    "# Detecting Outliers using IQR\n",
    "Q1 = df['Salary'].quantile(0.25)\n",
    "Q3 = df['Salary'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "\n",
    "df = df[(df['Salary'] >= Q1 - 1.5 * IQR) & (df['Salary'] <= Q3 + 1.5 * IQR)]\n",
    "\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **1.4 Standardizing Data Formats**\n",
    "\n",
    "### **Why Standardization of Data Formats is Important?**\n",
    "\n",
    "- Ensures Consistency: Data often comes in different formats, making uniform processing essential.\n",
    "\n",
    "- Facilitates Comparisons: Uniform formats enable accurate analysis and computations.\n",
    "\n",
    "- Reduces Errors: Inconsistent formats can lead to misinterpretations and processing issues.\n",
    "\n",
    "- Improves Data Quality: Standardization simplifies data cleaning and validation.\n",
    "\n",
    "### **Common Data Formats to Standardize**\n",
    "\n",
    "1. **Date and Time Standardization:** Standardizing date formats ensures accurate sorting, filtering, and time-based analysis. Using a universally accepted format, such as ISO 8601, enhances consistency.\n",
    "\n",
    "2. **Numeric Data Standardization:** Removing extra characters like currency symbols and ensuring a consistent decimal notation is essential for correct calculations.\n",
    "\n",
    "3. **Categorical Data Standardization:** Variations in categorical values (e.g., different capitalizations or abbreviations) can cause inconsistencies. Standardizing these values improves reliability.\n",
    "\n",
    "4. **Text and String Formatting:** Removing unnecessary spaces, special characters, and ensuring uniform capitalization enhances text processing.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Standardizing date format\n",
    "df['Join_Date'] = pd.to_datetime(df['Join_Date'], errors='coerce')\n",
    "\n",
    "# Converting currency column to numeric using different approaches\n",
    "df['Bonus'] = df['Bonus'].replace({r'\\$': '', r'€': ''}, regex=True).astype(int)\n",
    "\n",
    "# Standardizing numeric data\n",
    "df['Salary'] = df['Salary'].astype(int)\n",
    "df['Job_Satisfaction'] = df['Job_Satisfaction'].astype(int)\n",
    "df['Work_Experience'] = df['Work_Experience'].astype(int)\n",
    "\n",
    "df['Name'] = df['Name'].str.strip().str.title()\n",
    "\n",
    "\n",
    "\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **2. Data Pre-processing**\n",
    "\n",
    "## **2.1 Encoding Categorical Variables**\n",
    "\n",
    "### **Why do we need Encoding?**\n",
    "- Machine learning models require **numerical data**.\n",
    "- Encoding converts categorical values into numbers.\n",
    "\n",
    "### **Types of Encoding:**\n",
    "\n",
    "1. **Label Encoding**:\n",
    "   - Assigns **a unique integer** to each category.\n",
    "   - **Best for** ordinal categorical data (e.g., Small, Medium, Large).\n",
    "\n",
    "2. **One-Hot Encoding**:\n",
    "   - Creates separate **binary columns** for each category.\n",
    "   - **Best for** nominal categorical data (e.g., Cities, Colors).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Encoding categorical variables using multiple techniques\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "# Label Encoding for 'Education_Level' and 'Department'\n",
    "\n",
    "label_encoder.fit(df[\"Education_Level\"])\n",
    "education_mapping = dict(zip(df[\"Education_Level\"].unique(), label_encoder.transform(df[\"Education_Level\"].unique())))\n",
    "df[\"Education_Level\"] = label_encoder.transform(df[\"Education_Level\"])\n",
    "# df['Education_Level'] = label_encoder.fit_transform(df['Education_Level'])\n",
    "\n",
    "\n",
    "label_encoder.fit(df[\"Department\"])\n",
    "department_mapping = dict(zip(df[\"Department\"].unique(), label_encoder.transform(df[\"Department\"].unique())))\n",
    "df[\"Department\"] = label_encoder.transform(df[\"Department\"])\n",
    "# df['Department'] = label_encoder.fit_transform(df['Department'])\n",
    "\n",
    "\n",
    "# One-Hot Encode 'Remote_Work' and drop the original column\n",
    "df = pd.get_dummies(df, columns=[\"Remote_Work\"], drop_first=True)\n",
    "\n",
    "print(\"\\nEducation Level Mapping:\")\n",
    "education_df = pd.DataFrame(list(education_mapping.items()), columns=[\"Original Value\", \"Encoded Value\"])\n",
    "print(education_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nDepartment Mapping:\")\n",
    "department_df = pd.DataFrame(list(department_mapping.items()), columns=[\"Original Value\", \"Encoded Value\"])\n",
    "print(department_df.to_string(index=False))\n",
    "\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **2.1 Handling Imbalanced Datasets**\n",
    "\n",
    "- **Why It Matters**: In imbalanced datasets, one class significantly outweighs others, leading to biased models that favor the dominant class.\n",
    "- **Strategies for Handling Imbalance**:\n",
    "  - **Resampling Methods**: Oversampling the minority class or undersampling the majority class can help balance the dataset.\n",
    "  - **Synthetic Data Generation**: Techniques like SMOTE (Synthetic Minority Over-sampling Technique) generate synthetic examples to balance class distribution.\n",
    "  - **Algorithmic Approaches**: Some machine learning models handle imbalance better by adjusting class weights.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = df.dropna()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate Years at Company\n",
    "df[\"Years_at_Company\"] = (datetime.now() - df[\"Join_Date\"]).dt.days / 365\n",
    "\n",
    "# Drop the original Join_Date column\n",
    "df = df.drop(columns=[\"Join_Date\"])\n",
    "\n",
    "# Drop irrelevant columns\n",
    "df = df.drop(columns=[\"ID\", \"Name\"])  # Keeping only numerical features\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=[\"Performance_Score\"])  # Features\n",
    "y = df[\"Performance_Score\"]  # Target variable\n",
    "\n",
    "# Splitting dataset before SMOTE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(sampling_strategy=\"auto\", random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check class distribution after SMOTE\n",
    "print(\"Original class distribution:\\n\", y_train.value_counts())\n",
    "print(\"Resampled class distribution:\\n\", pd.Series(y_resampled).value_counts())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **2.2 Feature Selection**\n",
    "- **Why It Matters**: Irrelevant or low-variance features can add noise and reduce model performance.\n",
    "- **Methods for Feature Selection**:\n",
    "  - **Variance Thresholding**: Removing features with very low variance helps eliminate those that contribute little to predictive power.\n",
    "  - **Correlation Analysis**: Highly correlated features can be redundant and may be removed to simplify the model.\n",
    "  - **Model-Based Selection**: Feature importance"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Feature selection: Dropping low-variance columns manually\n",
    "\n",
    "numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "\n",
    "threshold = 0.01 * (1 - 0.01)\n",
    "low_variance_cols = [col for col in numerical_columns if df[col].var() < threshold]\n",
    "df.drop(columns=low_variance_cols, inplace=True)\n",
    "\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Data Transformation**\n",
    "\n",
    "## **3.1 Scaling & Normalization**\n",
    "\n",
    "### **Why Scale Data?**\n",
    "- Ensures that **all features contribute equally** to the model.\n",
    "- Improves performance in algorithms like KNN, SVM, and PCA.\n",
    "- Helps in handling skewed data for better model interpretability.\n",
    "\n",
    "### **Methods:**\n",
    "1. **Min-Max Scaling**: Scales data to a range **[0,1]**, preserving relative distances. Useful for models sensitive to feature magnitude.\n",
    "2. **Standardization (Z-score)**: Centers data around **mean = 0, std = 1**, making it suitable for normally distributed features.\n",
    "3. **Log Transformation**: Reduces skewness in features with highly skewed distributions, improving model robustness.\n",
    "4. **Feature Engineering**: Creating new meaningful features, such as ratios, to enhance predictive power.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "df[['Salary', 'Work_Experience']] = scaler.fit_transform(df[['Salary', 'Work_Experience']])\n",
    "\n",
    "\n",
    "# Standardization\n",
    "# df[['Salary', 'Work_Experience']] = StandardScaler().fit_transform(df[['Salary', 'Work_Experience']])\n",
    "\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Log transformation for skewed data using numpy\n",
    "for col in ['Salary', 'Bonus']:\n",
    "    df[col] = np.log1p(df[col])\n",
    "\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Feature engineering: Creating new features\n",
    "df['Experience per Year'] = df['Work_Experience'] / (df['Age'] + 1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.to_csv(\"./data/cleaned_synthetic_data.csv\", index=False)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  },
  "prev_pub_hash": "0f2053140168387dde22a52dafe4fef4f297eb98b70601c31c5dbae85ffb8130"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
